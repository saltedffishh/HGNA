import argparse
import os
import glob
import pandas as pd
import sys
import re

def get_project_root():
    """è·å–é¡¹ç›®æ ¹ç›®å½•"""
    current_file_path = os.path.abspath(__file__)
    src_dir = os.path.dirname(current_file_path)
    return os.path.dirname(src_dir)

def natural_sort_key(filepath):
    """
    è‡ªå®šä¹‰æ’åºè§„åˆ™ï¼š
    1. 'Asymptomatic' æ’åœ¨æœ€å‰é¢ (-1)
    2. å…¶ä»–æ–‡ä»¶æŒ‰æ–‡ä»¶åä¸­çš„ç¬¬ä¸€ä¸ªæ•°å­—å¤§å°æ’åº
    """
    filename = os.path.basename(filepath)
    
    # ç‰¹æ®Šå¤„ç†ï¼šAsymptomatic è§†ä¸ºæœ€ä¼˜å…ˆ
    if "Asymptomatic" in filename:
        return -1
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ–‡ä»¶åé‡Œçš„ç¬¬ä¸€ä¸ªè¿ç»­æ•°å­—
    # ä¾‹å¦‚ 'Days_1_4...' -> æå–å‡º 1
    # 'Days_14...' -> æå–å‡º 14
    numbers = re.findall(r'\d+', filename)
    
    if numbers:
        return int(numbers[0]) # è¿”å›æ•°å­—ç”¨äºæ¯”è¾ƒ
    else:
        return 999 # å¦‚æœæ²¡æ•°å­—ï¼Œæ’åœ¨æœ€å
        
def load_dataset_pairs(dataset_name):
    """
    è¯»å–æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„ matrix(.txt) å’Œ metadata(.csv)
    è¿”å›: (expr_list, meta_list, filenames)
    """
    root_dir = get_project_root()
    
    # 1. è·¯å¾„åŒ¹é…
    possible_paths = [
        os.path.join(root_dir, dataset_name),
        os.path.join(root_dir, f"{dataset_name}_data")
    ]
    target_path = next((p for p in possible_paths if os.path.exists(p)), None)
            
    if not target_path:
        print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ°ç›®å½• '{dataset_name}'")
        sys.exit(1)

    # 2. æœç´¢ txt æ–‡ä»¶
    search_pattern = os.path.join(target_path, "*_scaledata.txt")
    txt_files = glob.glob(search_pattern)

    if not txt_files:
        print(f"âŒ é”™è¯¯: {target_path} ä¸­æ²¡æœ‰æ‰¾åˆ° *_scaledata.txt")
        sys.exit(1)

    # 3. å…³é”®æ­¥éª¤ï¼šåº”ç”¨è‡ªå®šä¹‰æ’åº (æ•°å­—å¤§å°æ’åº)
    # è¿™ä¼šæŠŠ [Days_14, Days_5] å˜æˆ [Days_5, Days_14]
    txt_files.sort(key=natural_sort_key)

    print(f"ğŸ“‚ æ­£åœ¨ä»ç›®å½•åŠ è½½æ•°æ®: {os.path.basename(target_path)}")
    print(f"   æ’åºç­–ç•¥: Asymptomatic -> æ•°å­—ä»å°åˆ°å¤§")

    expr_list = [] # å­˜æ”¾è¡¨è¾¾çŸ©é˜µ
    meta_list = [] # å­˜æ”¾å…ƒæ•°æ®
    file_names = []

    # 4. å¾ªç¯è¯»å–æ–‡ä»¶å¯¹
    for txt_path in txt_files:
        base_name = os.path.basename(txt_path)
        
        # 4.1 æ¨æ–­å¯¹åº”çš„ CSV è·¯å¾„
        # å‡è®¾è§„åˆ™ï¼šXXX_scaledata.txt å¯¹åº” XXX_metadata.csv
        csv_name = base_name.replace("_scaledata.txt", "_metadata.csv")
        csv_path = os.path.join(target_path, csv_name)
        
        print(f"   â³ æ­£åœ¨è¯»å–ç»„: {base_name.split('_scaledata')[0]} ...", end="", flush=True)

        # 4.2 è¯»å– TXT (è¡¨è¾¾çŸ©é˜µ)
        # ä½¿ç”¨ä½ ç¡®è®¤è¿‡çš„å‚æ•°: sep=\s+
        expr = pd.read_csv(txt_path, sep=r"\s+", index_col=0)
        
        # 4.3 è¯»å– CSV (å…ƒæ•°æ®)
        # æ£€æŸ¥ csv æ˜¯å¦å­˜åœ¨
        if os.path.exists(csv_path):
            # ä½¿ç”¨ä½ æä¾›çš„ csv è¯»å–ä»£ç : é»˜è®¤ sep (é€—å·), index_col=0
            meta = pd.read_csv(csv_path, index_col=0)
        else:
            print(f"\n   âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ°å¯¹åº”çš„å…ƒæ•°æ® {csv_name}ï¼Œè¯¥ä½ç½®å¡«å……ä¸º None")
            meta = None
            
        # 4.4 å­˜å…¥åˆ—è¡¨
        expr_list.append(expr)
        meta_list.append(meta)
        file_names.append(base_name)
        
        print(f" âœ… (Expr: {expr.shape}, Meta: {meta.shape if meta is not None else 'Missing'})")

    return expr_list, meta_list, file_names

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('-e', '--experiment', type=str, required=True, help='æ•°æ®é›†åç§°')
    args = parser.parse_args()
    
    # è·å–ä¸¤ä¸ªåˆ—è¡¨
    expr_data, meta_data, names = load_dataset_pairs(args.experiment)
    
    # éªŒè¯æ’åºç»“æœ
    print("\nğŸ” æœ€ç»ˆåŠ è½½é¡ºåºéªŒè¯:")
    for i, name in enumerate(names):
        print(f"   [{i}] {name}")
        
    # ä½¿ç”¨ç¤ºä¾‹
    print("\nğŸ’¡ è°ƒç”¨ç¤ºä¾‹:")
    print("   expr_data[0] æ˜¯ç¬¬ä¸€ä¸ªæ—¶é—´ç‚¹çš„è¡¨è¾¾çŸ©é˜µ")
    print("   meta_data[0] æ˜¯ç¬¬ä¸€ä¸ªæ—¶é—´ç‚¹çš„å…ƒæ•°æ®")