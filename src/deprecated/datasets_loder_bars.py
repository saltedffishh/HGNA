import argparse
import os
import glob
import pandas as pd
import sys
import re
from concurrent.futures import ProcessPoolExecutor
from tqdm import tqdm

def get_project_root():
    """è·å–é¡¹ç›®æ ¹ç›®å½•"""
    current_file_path = os.path.abspath(__file__)
    src_dir = os.path.dirname(current_file_path)
    return os.path.dirname(src_dir)

def natural_sort_key(filepath):
    """æ’åºè§„åˆ™ï¼šAsymptomatic æœ€å‰ï¼Œå…¶ä½™æŒ‰æ•°å­—å¤§å°"""
    filename = os.path.basename(filepath)
    if "Asymptomatic" in filename:
        return -1
    numbers = re.findall(r'\d+', filename)
    return int(numbers[0]) if numbers else 999

# --- æ ¸å¿ƒä¿®æ”¹ï¼šå°†å•ä¸ªæ–‡ä»¶çš„è¯»å–é€»è¾‘å‰¥ç¦»æˆä¸€ä¸ªç‹¬ç«‹å‡½æ•° ---
# è¿™ä¸ªå‡½æ•°å¿…é¡»æ”¾åœ¨é¡¶å±‚ï¼Œä»¥ä¾¿å¤šè¿›ç¨‹è°ƒç”¨
def process_single_pair(txt_path):
    """
    å·¥ä½œå‡½æ•°ï¼šè¯»å–ä¸€å¯¹æ–‡ä»¶ (txt + csv)
    """
    base_name = os.path.basename(txt_path)
    
    # æ¨æ–­ CSV è·¯å¾„
    csv_name = base_name.replace("_scaledata.txt", "_metadata.csv")
    dir_name = os.path.dirname(txt_path)
    csv_path = os.path.join(dir_name, csv_name)
    
    # 1. è¯»å–çŸ©é˜µ (TXT)
    # æ³¨æ„ï¼šåœ¨å¤šè¿›ç¨‹æ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬é€šå¸¸ä¸æ˜¾ç¤ºå†…éƒ¨çš„ chunk è¿›åº¦æ¡ï¼Œ
    # å› ä¸º5ä¸ªè¿›åº¦æ¡æ··åœ¨ä¸€èµ·ä¼šæ‰“ä¹±å±å¹•æ˜¾ç¤ºã€‚
    try:
        expr = pd.read_csv(txt_path, sep=r"\s+", index_col=0)
    except Exception as e:
        return None, None, base_name, f"Error reading txt: {str(e)}"

    # 2. è¯»å–å…ƒæ•°æ® (CSV)
    if os.path.exists(csv_path):
        try:
            meta = pd.read_csv(csv_path, index_col=0)
        except Exception as e:
            meta = None
    else:
        meta = None
        
    return expr, meta, base_name, None # æœ€åä¸€ä¸ª None ä»£è¡¨æ— é”™è¯¯

def load_dataset_multicore(dataset_name, max_workers=4):
    """
    å¤šæ ¸åŠ è½½ä¸»å‡½æ•°
    """
    root_dir = get_project_root()
    
    # 1. è·¯å¾„å¯»æ‰¾
    possible_paths = [
        os.path.join(root_dir, dataset_name),
        os.path.join(root_dir, f"{dataset_name}_data")
    ]
    target_path = next((p for p in possible_paths if os.path.exists(p)), None)
            
    if not target_path:
        print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ°ç›®å½• '{dataset_name}'")
        sys.exit(1)

    # 2. æœç´¢å¹¶æ’åºæ–‡ä»¶
    search_pattern = os.path.join(target_path, "*_scaledata.txt")
    txt_files = glob.glob(search_pattern)
    txt_files.sort(key=natural_sort_key)

    if not txt_files:
        print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
        sys.exit(1)

    # 3. å‡†å¤‡å¤šè¿›ç¨‹
    # å¦‚æœæ–‡ä»¶æ•°å°‘äºæ ¸æ•°ï¼Œå°±æ²¡å¿…è¦å¼€é‚£ä¹ˆå¤šæ ¸
    actual_workers = min(len(txt_files), max_workers)
    
    print(f"ğŸš€ å¯åŠ¨å¤šæ ¸åŠ é€Ÿ: ä½¿ç”¨ {actual_workers} ä¸ªæ ¸å¿ƒå¹¶è¡ŒåŠ è½½ {len(txt_files)} ä¸ªæ–‡ä»¶...")
    print(f"ğŸ“‚ æ•°æ®æº: {os.path.basename(target_path)}")
    print("-" * 60)

    # 4. å¹¶è¡Œæ‰§è¡Œ
    results = []
    # ProcessPoolExecutor è‡ªåŠ¨ç®¡ç†è¿›ç¨‹æ± 
    with ProcessPoolExecutor(max_workers=actual_workers) as executor:
        # executor.map ä¼šæŒ‰ç…§ txt_files çš„è¾“å…¥é¡ºåºè¿”å›ç»“æœï¼Œè¿™éå¸¸é‡è¦ï¼
        # è¿™æ ·æˆ‘ä»¬å°±ä¸éœ€è¦é‡æ–°æ’åºäº†ï¼Œåªè¦è¾“å…¥æ˜¯æ’å¥½åºçš„ï¼Œè¾“å‡ºå°±æ˜¯æ’å¥½åºçš„ã€‚
        # tqdm ç”¨äºæ˜¾ç¤ºâ€œå®Œæˆäº†å‡ ä¸ªæ–‡ä»¶â€
        results_generator = list(tqdm(
            executor.map(process_single_pair, txt_files), 
            total=len(txt_files), 
            unit="file",
            desc="æ€»è¿›åº¦"
        ))

    # 5. è§£åŒ…ç»“æœ
    expr_list = []
    meta_list = []
    file_names = []

    for expr, meta, name, error in results_generator:
        if error:
            print(f"âŒ è¯»å– {name} å¤±è´¥: {error}")
            sys.exit(1)
            
        expr_list.append(expr)
        meta_list.append(meta)
        file_names.append(name)
        
        # ç®€å•æ‰“å°æ¯ä¸ªæ–‡ä»¶çš„ç»´åº¦
        meta_shape = meta.shape if meta is not None else "Missing"
        tqdm.write(f"   âœ… {name}: Matrix={expr.shape}, Meta={meta_shape}")

    print("-" * 60)
    return expr_list, meta_list, file_names

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å¤šæ ¸æ•°æ®åŠ è½½å™¨")
    parser.add_argument('-e', '--experiment', type=str, required=True, help='æ•°æ®é›†åç§°')
    
    # æ–°å¢å‚æ•°ï¼š-j æˆ– --jobs æŒ‡å®šæ ¸æ•°
    parser.add_argument('-j', '--jobs', type=int, default=4, help='ä½¿ç”¨çš„CPUæ ¸å¿ƒæ•° (é»˜è®¤: 4)')
    
    args = parser.parse_args()
    
    # è¿™é‡Œçš„ names åªæ˜¯ä¸ºäº†è®©ä½ çœ‹ä¸‹æ•ˆæœï¼Œå®é™…è¿”å›çš„å°±æ˜¯ä¸‰ä¸ªåˆ—è¡¨
    exprs, metas, names = load_dataset_multicore(args.experiment, max_workers=args.jobs)