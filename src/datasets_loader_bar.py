import argparse
import os
import glob
import pandas as pd
import sys
import re
from tqdm import tqdm

def get_project_root():
    """
    è·å–é¡¹ç›®æ ¹ç›®å½• (å‡è®¾è„šæœ¬ä½äº src/ ä¸‹ï¼Œé¡¹ç›®æ ¹ç›®å½•ä¸ºä¸Šä¸€çº§)
    """
    current_file_path = os.path.abspath(__file__)
    src_dir = os.path.dirname(current_file_path)
    return os.path.dirname(src_dir)

def natural_sort_key(filepath):
    """
    æ’åºè§„åˆ™ï¼š
    1. 'Asymptomatic' å¼ºåˆ¶æ’åœ¨æœ€å‰ (-1)
    2. å…¶ä»–æ–‡ä»¶æŒ‰æ–‡ä»¶åä¸­çš„ç¬¬ä¸€ä¸ªæ•°å­—å¤§å°æ’åº (1, 5, 9, 14...)
    """
    filename = os.path.basename(filepath)
    if "Asymptomatic" in filename:
        return -1
    numbers = re.findall(r'\d+', filename)
    return int(numbers[0]) if numbers else 999

def get_file_line_count(filepath):
    """
    å¿«é€Ÿè®¡ç®—æ–‡ä»¶æ€»è¡Œæ•°ï¼Œç”¨äºå®šä¹‰è¿›åº¦æ¡çš„æ€»é•¿åº¦ (Total)
    ä½¿ç”¨äºŒè¿›åˆ¶è¯»å–æ¨¡å¼ ('rb') ä»¥è·å¾—æœ€å¿«é€Ÿåº¦
    """
    with open(filepath, 'rb') as f:
        return sum(1 for _ in f)

def load_dataset_pairs(dataset_name):
    """
    åŠ è½½æŒ‡å®šæ•°æ®é›†ä¸‹çš„ matrix (.txt) å’Œ metadata (.csv)
    ç‰¹æ€§ï¼šæ”¯æŒå•æ–‡ä»¶å†…éƒ¨è¿›åº¦æ¡æ˜¾ç¤º (åˆ†å—è¯»å–)
    """
    root_dir = get_project_root() # è·å– HGNA æ ¹ç›®å½•
    
    # --- 1. æ™ºèƒ½è·¯å¾„åŒ¹é… (ä¿®æ”¹ç‰ˆ) ---
    # æˆ‘ä»¬å¢åŠ äº†å¯¹ "datasets" æ–‡ä»¶å¤¹çš„æœç´¢
    possible_paths = [
        # ä¼˜å…ˆçº§ 1: HGNA/datasets/COVID19 (è¿™æ˜¯ä½ ç°åœ¨çš„ç»“æ„)
        os.path.join(root_dir, "datasets", dataset_name),
        
        # ä¼˜å…ˆçº§ 2: HGNA/datasets/COVID19_data (é˜²æ­¢ä½ æ–‡ä»¶å¤¹åå­—å¸¦_dataåç¼€)
        os.path.join(root_dir, "datasets", f"{dataset_name}_data"),
        
        # ä¼˜å…ˆçº§ 3: å…¼å®¹æ—§æ¨¡å¼ (ç›´æ¥åœ¨æ ¹ç›®å½•ä¸‹æ‰¾)
        os.path.join(root_dir, dataset_name),
        os.path.join(root_dir, f"{dataset_name}_data")
    ]
    
    # è‡ªåŠ¨åœ¨ä¸Šé¢åˆ—è¡¨ä¸­å¯»æ‰¾ç¬¬ä¸€ä¸ªå­˜åœ¨çš„è·¯å¾„
    target_path = next((p for p in possible_paths if os.path.exists(p)), None)
            
    if not target_path:
        # æ›´æ–°æŠ¥é”™ä¿¡æ¯ï¼Œæç¤ºç”¨æˆ·æˆ‘ä»¬å» datasets æ‰¾è¿‡äº†
        print(f"âŒ é”™è¯¯: åœ¨ 'datasets' æ–‡ä»¶å¤¹æˆ–æ ¹ç›®å½•ä¸‹æœªæ‰¾åˆ° '{dataset_name}'")
        sys.exit(1)

    # --- 2. æœç´¢æ–‡ä»¶å¹¶æ’åº ---
    search_pattern = os.path.join(target_path, "*_scaledata.txt")
    txt_files = glob.glob(search_pattern)
    
    # åº”ç”¨è‡ªå®šä¹‰æ’åº
    txt_files.sort(key=natural_sort_key)

    if not txt_files:
        print(f"âŒ é”™è¯¯: åœ¨ {target_path} ä¸­æœªæ‰¾åˆ° *_scaledata.txt æ–‡ä»¶")
        sys.exit(1)

    # --- 3. åˆå§‹åŒ–å®¹å™¨ ---
    expr_list = []
    meta_list = []
    file_names = []

    print(f"ğŸ“‚ å‡†å¤‡åŠ è½½ {len(txt_files)} ä¸ªæ•°æ®é›†æ¥è‡ª: {os.path.basename(target_path)}")
    print("-" * 65)

    # --- 4. é€ä¸ªæ–‡ä»¶å¤„ç† ---
    for i, txt_path in enumerate(txt_files):
        base_name = os.path.basename(txt_path)
        display_name = base_name.split('_scaledata')[0] # æå–ç®€çŸ­åå­—ç”¨äºæ˜¾ç¤º
        
        # æ¨æ–­å¯¹åº”çš„ CSV è·¯å¾„
        csv_name = base_name.replace("_scaledata.txt", "_metadata.csv")
        csv_path = os.path.join(target_path, csv_name)

        print(f"[{i+1}/{len(txt_files)}] æ­£åœ¨è¯»å–: {display_name}")

        # === æ ¸å¿ƒï¼šåˆ†å—è¯»å– Matrix ä»¥æ˜¾ç¤ºè¿›åº¦ ===
        
        # 4.1 é¢„ä¼°æ–‡ä»¶å¤§å° (è®¡ç®—è¡Œæ•°)
        print(f"   â†³ æ­£åœ¨æ‰«ææ–‡ä»¶è¡Œæ•°...", end="\r")
        total_lines = get_file_line_count(txt_path)
        
        # 4.2 è®¾å®šåˆ†å—å¤§å°
        # ä½ çš„çŸ©é˜µæ˜¯ (3000è¡Œ x 100000åˆ—)ï¼Œæ„å‘³ç€æ¯æ¬¡ read_csv éœ€è¦å¤„ç†å¾ˆå®½çš„æ•°æ®
        # chunksize=100 è¡¨ç¤ºæ¯æ¬¡è¯»å– 100 ä¸ªåŸºå› ï¼ˆè¡Œï¼‰
        chunk_size = 100 
        
        chunks = []
        
        # 4.3 è¯»å–å¾ªç¯
        # index_col=0 è¡¨ç¤ºç¬¬ä¸€åˆ—æ˜¯åŸºå› å
        # sep=r"\s+" å¤„ç†ç©ºæ ¼æˆ–åˆ¶è¡¨ç¬¦åˆ†éš”
        try:
            with pd.read_csv(txt_path, sep=r"\s+", index_col=0, chunksize=chunk_size) as reader:
                # total_lines - 1 æ˜¯å› ä¸º header å æ®äº†ä¸€è¡Œï¼Œä½†è¿™åªæ˜¯ä¼°ç®—ï¼Œä¸å‡ä¹Ÿè¡Œ
                with tqdm(total=total_lines, unit="row", desc="   â†³ è¿›åº¦", ncols=80, leave=True) as pbar:
                    for chunk in reader:
                        chunks.append(chunk)
                        pbar.update(len(chunk)) # æ›´æ–°è¿›åº¦æ¡
            
            # 4.4 åˆå¹¶å—
            expr = pd.concat(chunks)
            
        except Exception as e:
            print(f"\nâŒ è¯»å–å¤±è´¥: {e}")
            sys.exit(1)

        # === è¯»å– Metadata ===
        if os.path.exists(csv_path):
            try:
                meta = pd.read_csv(csv_path, index_col=0)
            except Exception as e:
                print(f"   âš ï¸  è¯»å– CSV å‡ºé”™: {e}")
                meta = None
        else:
            print(f"   âš ï¸  è­¦å‘Š: ç¼ºå¤± Metadata CSV")
            meta = None
        
        # å­˜å…¥åˆ—è¡¨
        expr_list.append(expr)
        meta_list.append(meta)
        file_names.append(base_name)
        
        # æ‰“å°å®Œæˆä¿¡æ¯
        print(f"   âœ… å®Œæˆ. Matrix Shape: {expr.shape}\n")

    return expr_list, meta_list, file_names

if __name__ == "__main__":
    # --- å‘½ä»¤è¡Œå…¥å£ ---
    parser = argparse.ArgumentParser(description="å•ç»†èƒæ•°æ®åŠ è½½å™¨ (å¸¦è¯¦ç»†è¿›åº¦æ¡)")
    parser.add_argument('-e', '--experiment', type=str, required=True, help='æ•°æ®é›†åç§° (ä¾‹å¦‚: COVID19)')
    
    args = parser.parse_args()
    
    # è°ƒç”¨å‡½æ•°
    exprs, metas, names = load_dataset_pairs(args.experiment)
    
    print("ğŸ‰ æ‰€æœ‰æ•°æ®åŠ è½½å®Œæ¯•ï¼")